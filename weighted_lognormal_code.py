# -*- coding: utf-8 -*-
"""weighted lognormal  code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vis1RSQBpEmXbfNKxibB7TtTmY86zXMc

graphs of wpdf and wcdf
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import lognorm
from scipy.integrate import cumulative_trapezoid

# --- Weighted Lognormal PDF & CDF definitions ---
def weighted_lognorm_pdf(x, mu, sigma, c):
    f_ln = lognorm.pdf(x, s=sigma, scale=np.exp(mu))
    Ex_c = np.exp(c * mu + 0.5 * (c * sigma) ** 2)
    return (x**c * f_ln) / Ex_c

def weighted_lognorm_cdf(x, mu, sigma, c):
    pdf_vals = weighted_lognorm_pdf(x, mu, sigma, c)
    cdf_vals = cumulative_trapezoid(pdf_vals, x, initial=0)
    cdf_vals /= cdf_vals[-1]
    return cdf_vals

# --- x range and parameter sets ---
x = np.linspace(0.01, 30, 500)

# Vary parameters
mu_values = [1.0, 1.5]
sigma_values = [0.3, 1.5]
c_values = [0, 1, 2]

# --- Combined PDF plot ---
plt.figure(figsize=(9, 6))

# Vary mu
for mu in mu_values:
    plt.plot(x, weighted_lognorm_pdf(x, mu, 0.6, 1.0), linestyle='-', label=f"μ={mu}, σ=0.6, c=1")

# Vary sigma
for sigma in sigma_values:
    plt.plot(x, weighted_lognorm_pdf(x, 2.0, sigma, 1.0), linestyle='--', label=f"μ=2, σ={sigma}, c=1")

# Vary c
for c in c_values:
    plt.plot(x, weighted_lognorm_pdf(x, 2.0, 0.6, c), linestyle=':', label=f"μ=2, σ=0.6, c={c}")

plt.title("Weighted Lognormal PDF")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend(fontsize=8)
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Combined CDF plot ---
plt.figure(figsize=(9, 6))

# Vary mu
for mu in mu_values:
    plt.plot(x, weighted_lognorm_cdf(x, mu, 0.6, 1.0), linestyle='-', label=f"μ={mu}, σ=0.6, c=1")

# Vary sigma
for sigma in sigma_values:
    plt.plot(x, weighted_lognorm_cdf(x, 2.0, sigma, 1.0), linestyle='--', label=f"μ=2, σ={sigma}, c=1")

# Vary c
for c in c_values:
    plt.plot(x, weighted_lognorm_cdf(x, 2.0, 0.6, c), linestyle=':', label=f"μ=2, σ=0.6, c={c}")

plt.title("Weighted Lognormal CDF")
plt.xlabel("x")
plt.ylabel("Cumulative Probability")
plt.legend(fontsize=8)
plt.grid(True)
plt.tight_layout()
plt.show()

"""ex 6 - fits p value 0.8632"""

import numpy as np
from scipy.optimize import minimize
from scipy.stats import kstest, norm
import matplotlib.pyplot as plt

# Dataset: Failure times (30 observations)
x = np.array([23, 261, 87, 7, 120, 14, 62, 47, 225, 71, 246, 21, 42, 20, 5, 12, 120, 11, 3, 14,
              71, 11, 14, 11, 16, 90, 1, 16, 52, 95])

n = len(x)

# Weighted Lognormal PDF
def weighted_lognormal_pdf(x, mu, sigma, c):
    return (c / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-((c * np.log(x) - mu)**2) / (2 * sigma**2))

# Weighted Lognormal CDF (for KS test)
def weighted_lognormal_cdf(x, mu, sigma, c):
    return norm.cdf(c * np.log(x), loc=mu, scale=sigma)

# Negative Log-Likelihood
def neg_log_likelihood(params):
    mu, sigma, c = params
    if sigma <= 0 or c <= 0:
        return np.inf
    pdf = weighted_lognormal_pdf(x, mu, sigma, c)
    # Avoid log(0) issues
    pdf = np.where(pdf > 0, pdf, 1e-12)
    return -np.sum(np.log(pdf))

# Initial guesses (log-mean, log-std, and c)
initial = [np.mean(np.log(x[x > 0])), np.std(np.log(x[x > 0])), 0.5]

# MLE Optimization
result = minimize(neg_log_likelihood, initial, method='L-BFGS-B',
                  bounds=[(None, None), (1e-6, None), (1e-6, None)])
mu_hat, sigma_hat, c_hat = result.x

# Log-likelihood value
log_likelihood = -result.fun
k = 3  # number of parameters (mu, sigma, c)

# Information Criteria
AIC = 2 * k - 2 * log_likelihood
BIC = k * np.log(n) - 2 * log_likelihood
AICc = AIC + (2 * k * (k + 1)) / (n - k - 1)
HQIC = 2 * k * np.log(np.log(n)) - 2 * log_likelihood

# Display Results
print("=== MLE Parameter Estimates ===")
print(f"μ (mu) = {mu_hat:.4f}")
print(f"σ (sigma) = {sigma_hat:.4f}")
print(f"c (weighting parameter) = {c_hat:.4f}")

print("\n=== Model Evaluation ===")
print(f"Log-Likelihood = {log_likelihood:.4f}")
print(f"AIC  = {AIC:.4f}")
print(f"BIC  = {BIC:.4f}")
print(f"AICc = {AICc:.4f}")
print(f"HQIC = {HQIC:.4f}")

# KS test for goodness-of-fit
ks_stat, p_value = kstest(x, lambda v: weighted_lognormal_cdf(v, mu_hat, sigma_hat, c_hat))
print("\n=== Kolmogorov–Smirnov Test ===")
print(f"KS Statistic = {ks_stat:.4f}")
print(f"p-value      = {p_value:.4f}")

# Plot fitted PDF vs data histogram
x_range = np.linspace(min(x)*0.9, max(x)*1.1, 500)
pdf_vals = weighted_lognormal_pdf(x_range, mu_hat, sigma_hat, c_hat)

plt.hist(x, bins=10, density=True, alpha=0.5, color='skyblue', edgecolor='black', label='Observed Data')
plt.plot(x_range, pdf_vals, 'r-', lw=2, label='Fitted Data')
plt.xlabel('Failure Time')
plt.ylabel('Density')
plt.title('Weighted Lognormal Fit with Estimated Parameters')
plt.legend()
plt.show()

"""ex 7 fits p value 0.5584"""

import numpy as np
from scipy.optimize import minimize
from scipy.stats import kstest, norm
import matplotlib.pyplot as plt

# Dataset: Failure times-II (20 observations)
x = np.array([0.067, 0.068, 0.076, 0.081, 0.084, 0.085, 0.085, 0.086, 0.089, 0.098,
              0.098, 0.114, 0.114, 0.115, 0.121, 0.125, 0.131, 0.149, 0.160, 0.485])

n = len(x)

# Weighted Lognormal PDF
def weighted_lognormal_pdf(x, mu, sigma, c):
    return (c / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-((c * np.log(x) - mu)**2) / (2 * sigma**2))

# Weighted Lognormal CDF (for KS test)
def weighted_lognormal_cdf(x, mu, sigma, c):
    return norm.cdf(c * np.log(x), loc=mu, scale=sigma)

# Negative Log-Likelihood Function
def neg_log_likelihood(params):
    mu, sigma, c = params
    if sigma <= 0 or c <= 0:
        return np.inf
    pdf = weighted_lognormal_pdf(x, mu, sigma, c)
    pdf = np.where(pdf > 0, pdf, 1e-12)  # avoid log(0)
    return -np.sum(np.log(pdf))

# Initial guesses
initial = [np.mean(np.log(x[x > 0])), np.std(np.log(x[x > 0])), 0.5]

# MLE optimization
result = minimize(neg_log_likelihood, initial, method='L-BFGS-B',
                  bounds=[(None, None), (1e-6, None), (1e-6, None)])

mu_hat, sigma_hat, c_hat = result.x
log_likelihood = -result.fun
k = 3  # number of parameters

# Information Criteria
AIC = 2 * k - 2 * log_likelihood
BIC = k * np.log(n) - 2 * log_likelihood
AICc = AIC + (2 * k * (k + 1)) / (n - k - 1)
HQIC = 2 * k * np.log(np.log(n)) - 2 * log_likelihood

# Display results
print("=== MLE Parameter Estimates ===")
print(f"μ (mu) = {mu_hat:.4f}")
print(f"σ (sigma) = {sigma_hat:.4f}")
print(f"c (weighting parameter) = {c_hat:.4f}")

print("\n=== Model Evaluation ===")
print(f"Log-Likelihood = {log_likelihood:.4f}")
print(f"AIC  = {AIC:.4f}")
print(f"BIC  = {BIC:.4f}")
print(f"AICc = {AICc:.4f}")
print(f"HQIC = {HQIC:.4f}")

# Kolmogorov–Smirnov Test
ks_stat, p_value = kstest(x, lambda v: weighted_lognormal_cdf(v, mu_hat, sigma_hat, c_hat))
print("\n=== Kolmogorov–Smirnov Test ===")
print(f"KS Statistic = {ks_stat:.4f}")
print(f"p-value      = {p_value:.4f}")

# Plot: Histogram vs. Fitted Weighted Lognormal PDF
x_range = np.linspace(min(x)*0.9, max(x)*1.1, 500)
pdf_vals = weighted_lognormal_pdf(x_range, mu_hat, sigma_hat, c_hat)

plt.hist(x, bins=10, density=True, alpha=0.5, color='lightgreen', edgecolor='black', label='Observed Data')
plt.plot(x_range, pdf_vals, 'r-', lw=2, label='Fitted Data')
plt.xlabel('Failure Time')
plt.ylabel('Density')
plt.title('Weighted Lognormal Fit (Failure Times-II)')
plt.legend()
plt.show()

"""ex 8 - fits p value 0.9687"""

import numpy as np
from scipy.optimize import minimize
from scipy.stats import kstest, norm
import matplotlib.pyplot as plt

# Dataset: Ugarte et al. (38 observations)
x = np.array([1501.82, 6989.43, 2424.02, 4150.29, 8693.35, 2643.77, 13148.37, 6149.39, 23587.21,
              7248.37, 4788.22, 6009.51, 5349.65, 5741.32, 7065.81, 7261.37, 2358.42, 10357.88,
              2499.05, 3022.90, 4234.86, 4482.03, 6363.71, 3329.91, 8740.47, 3664.95, 4515.97,
              8497.71, 4569.89, 8069.63, 7366.79, 1525.41, 3363.02, 2420.57, 3576.74, 3708.05,
              5819.12, 5479.38])

# Weighted lognormal PDF
def weighted_lognormal_pdf(x, mu, sigma, c):
    return (c / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-((c * np.log(x) - mu)**2) / (2 * sigma**2))

# Weighted lognormal CDF (for KS test)
def weighted_lognormal_cdf(x, mu, sigma, c):
    return norm.cdf(c * np.log(x), loc=mu, scale=sigma)

# Negative log-likelihood
def neg_log_likelihood(params):
    mu, sigma, c = params
    if sigma <= 0 or c <= 0:   # enforce positive sigma and c
        return np.inf
    pdf = weighted_lognormal_pdf(x, mu, sigma, c)
    return -np.sum(np.log(pdf))

# Initial guesses (log-mean and std, c unknown)
initial = [np.mean(np.log(x)), np.std(np.log(x)), 0.5]

# MLE optimization
result = minimize(neg_log_likelihood, initial, method='L-BFGS-B',
                  bounds=[(None,None),(1e-6,None),(1e-6,None)])
mu_hat, sigma_hat, c_hat = result.x

# Log-likelihood
log_likelihood = -neg_log_likelihood([mu_hat, sigma_hat, c_hat])
n = len(x)
k = 3  # number of parameters

# Information criteria
aic = -2 * log_likelihood + 2 * k
bic = -2 * log_likelihood + k * np.log(n)
aicc = aic + (2 * k * (k + 1)) / (n - k - 1)
hqic = -2 * log_likelihood + 2 * k * np.log(np.log(n))

# Display results
print("Estimated Parameters via MLE:")
print(f"μ (mu) = {mu_hat:.4f}")
print(f"σ (sigma) = {sigma_hat:.4f}")
print(f"c (weighting parameter) = {c_hat:.4f}")

print("\nModel Fit Statistics:")
print(f"Log-Likelihood = {log_likelihood:.4f}")
print(f"AIC  = {aic:.4f}")
print(f"AICC = {aicc:.4f}")
print(f"BIC  = {bic:.4f}")
print(f"HQIC = {hqic:.4f}")

# KS test
ks_stat, p_value = kstest(x, lambda v: weighted_lognormal_cdf(v, mu_hat, sigma_hat, c_hat))
print(f"\nKolmogorov-Smirnov Test:")
print(f"KS statistic = {ks_stat:.4f}")
print(f"p-value = {p_value:.4f}")

# Plot histogram and fitted PDF
x_range = np.linspace(min(x)*0.9, max(x)*1.1, 500)
pdf_vals = weighted_lognormal_pdf(x_range, mu_hat, sigma_hat, c_hat)

plt.hist(x, bins=12, density=True, alpha=0.5, color='lightcoral', edgecolor='black', label='Observed Data')
plt.plot(x_range, pdf_vals, 'r-', lw=2, label='Fitted Data')
plt.xlabel('Observation')
plt.ylabel('Density')
plt.title('Weighted Lognormal Fit with Estimated Parameters')
plt.legend()
plt.show()

"""Simulation part"""

import numpy as np
import pandas as pd
from scipy.optimize import minimize

# --------------------------
# Weighted Lognormal Negative Log-Likelihood
# --------------------------
def weighted_lognormal_negloglik(params, data):
    mu, sigma, c = params
    if sigma <= 0 or c <= 0:
        return np.inf
    x = np.array(data)
    pdf_vals = (c / (x * sigma * np.sqrt(2 * np.pi))) * np.exp(-((c * np.log(x) - mu)**2) / (2 * sigma**2))
    pdf_vals = np.clip(pdf_vals, 1e-12, None)
    return -np.sum(np.log(pdf_vals))

def mle_estimate(data):
    init_params = [np.log(np.mean(data)), np.std(np.log(data)), 1.0]
    res = minimize(weighted_lognormal_negloglik, init_params, args=(data,),
                   method='L-BFGS-B', bounds=[(None, None), (1e-6, None), (1e-6, None)])
    return res.x

# --------------------------
# Datasets
# --------------------------
datasets = {
    "Failure Times I": [23, 261, 87, 7, 120, 14, 62, 47, 225, 71, 246, 21,
                        42, 20, 5, 12, 120, 11, 3, 14, 71, 11, 14, 11, 16, 90, 1, 16, 52, 95],

    "Failure Times II": [0.067, 0.068, 0.076, 0.081, 0.084, 0.085, 0.085, 0.086,
                         0.089, 0.098, 0.098, 0.114, 0.114, 0.115, 0.121, 0.125,
                         0.131, 0.149, 0.160, 0.485],

    "Real Dataset": [1501.82, 6989.43, 2424.02, 4150.29, 8693.35, 2643.77, 13148.37, 6149.39,
                     23587.21, 7248.37, 4788.22, 6009.51, 5349.65, 5741.32, 7065.81, 7261.37,
                     2358.42, 10357.88, 2499.05, 3022.90, 4234.86, 4482.03, 6363.71, 3329.91,
                     8740.47, 3664.95, 4515.97, 8497.71, 4569.89, 8069.63, 7366.79, 1525.41,
                     3363.02, 2420.57, 3576.74, 3708.05, 5819.12, 5479.38]
}

# --------------------------
# Simulation Parameters
# --------------------------
sample_sizes = [20, 30, 40, 50, 100]
replications = 1000  # You can increase to 1000 for publication-quality results

# --------------------------
# Simulation Loop
# --------------------------
all_tables = {}

for name, data in datasets.items():
    np.random.seed(42)
    full_params = mle_estimate(data)
    mu_true, sigma_true, c_true = full_params

    results = []
    for n in sample_sizes:
        mu_hats, sigma_hats, c_hats = [], [], []
        for _ in range(replications):
            sample = np.random.choice(data, size=min(n, len(data)), replace=True)
            mu_hat, sigma_hat, c_hat = mle_estimate(sample)
            mu_hats.append(mu_hat)
            sigma_hats.append(sigma_hat)
            c_hats.append(c_hat)

        mu_hats, sigma_hats, c_hats = np.array(mu_hats), np.array(sigma_hats), np.array(c_hats)

        # Bias, SE, MSE
        bias_mu = np.mean(mu_hats - mu_true)
        bias_sigma = np.mean(sigma_hats - sigma_true)
        bias_c = np.mean(c_hats - c_true)

        se_mu = np.std(mu_hats, ddof=1)
        se_sigma = np.std(sigma_hats, ddof=1)
        se_c = np.std(c_hats, ddof=1)

        mse_mu = np.mean((mu_hats - mu_true)**2)
        mse_sigma = np.mean((sigma_hats - sigma_true)**2)
        mse_c = np.mean((c_hats - c_true)**2)

        results.append({
            "μ (True)": mu_true,
            "σ (True)": sigma_true,
            "c (True)": c_true,
            "n": n,
            "μ̂": np.mean(mu_hats),
            "σ̂": np.mean(sigma_hats),
            "ĉ": np.mean(c_hats),
            "SE(μ̂)": se_mu,
            "SE(σ̂)": se_sigma,
            "SE(ĉ)": se_c,
            "Bias(μ̂)": bias_mu,
            "Bias(σ̂)": bias_sigma,
            "Bias(ĉ)": bias_c,
            "MSE(μ̂)": mse_mu,
            "MSE(σ̂)": mse_sigma,
            "MSE(ĉ)": mse_c
        })

    df = pd.DataFrame(results)
    all_tables[name] = df.round(6)

# --------------------------
# Display Tables (screenshot-style)
# --------------------------
for name, table in all_tables.items():
    print(f"\n{'='*100}")
    print(f"Table: Simulation results for Weighted Lognormal MLEs - {name}")
    print(f"{'='*100}")
    print(table.to_string(index=False))

"""quantile function"""

import numpy as np
import pandas as pd
from scipy.stats import norm

# Weighted lognormal quantile function
def weighted_lognormal_quantile(p, mu, sigma, c):
    """
    Quantile function (inverse CDF) of the weighted lognormal distribution.
    Q_w(p) = exp(mu + c*sigma^2 + sigma * Phi^{-1}(p))
    """
    p = np.asarray(p)
    return np.exp(mu + c * sigma**2 + sigma * norm.ppf(p))

# Parameters
mu = 0.5
sigma = 0.8
c_values = [-1.0, 0.0, 0.5, 1.0]
percentiles = [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99]

# Create table
table = {}
for c in c_values:
    table[f"c={c}"] = weighted_lognormal_quantile(percentiles, mu, sigma, c)

df = pd.DataFrame(table, index=[f"{int(100*p)}%" for p in percentiles])
print("Weighted Lognormal Quantiles (mu=0.5, sigma=0.8):\n")
print(df)

# Example single quantiles for c=0.5
print("\nExample values for c=0.5:")
for p in [0.05, 0.5, 0.95]:
    q = weighted_lognormal_quantile(p, mu, sigma, 0.5)
    print(f"  Q_w({p}) = {q:.6f}")

"""Finding moments"""

import numpy as np
import pandas as pd
from scipy.stats import skew, kurtosis

# Weighted Lognormal random sample generator
def weighted_lognormal_rvs(mu, sigma, c, size=100000):
    z = np.random.lognormal(mean=mu, sigma=sigma, size=size)
    weights = np.exp(-c * z)  # weight function (example)
    weights /= np.sum(weights)
    return np.random.choice(z, size=size, p=weights)

# Function to compute statistical moments
def compute_moments(mu, sigma, c):
    x = weighted_lognormal_rvs(mu, sigma, c)
    E1 = np.mean(x)
    E2 = np.mean(x**2)
    E3 = np.mean(x**3)
    E4 = np.mean(x**4)
    Var = E2 - E1**2
    Sk = skew(x)
    Ku = kurtosis(x, fisher=False)  # Pearson kurtosis
    return [E1, E2, E3, E4, Var, Sk, Ku]

# === TABLE 1:
mu = 0.5
c = 1.0
sigmas = np.arange(1, 5.6, 0.5)
data_sigma = [compute_moments(mu, s, c) for s in sigmas]
table_sigma = pd.DataFrame(data_sigma, columns=['E(x)', 'E(x)^2', 'E(x)^3', 'E(x)^4', 'V(x)', 'Skewness', 'Kurtosis'])
table_sigma.insert(0, 'σ', np.round(sigmas, 3))

# === TABLE 2:
sigma = 1.0
c = 1.0
mus = np.arange(1, 5.6, 0.5)
data_mu = [compute_moments(m, sigma, c) for m in mus]
table_mu = pd.DataFrame(data_mu, columns=['E(x)', 'E(x)^2', 'E(x)^3', 'E(x)^4', 'V(x)', 'Skewness', 'Kurtosis'])
table_mu.insert(0, 'μ', np.round(mus, 3))

# === TABLE 3:
mu = 0.5
sigma = 1.0
cs = np.arange(1, 5.6, 0.5)
data_c = [compute_moments(mu, sigma, c_val) for c_val in cs]
table_c = pd.DataFrame(data_c, columns=['E(x)', 'E(x)^2', 'E(x)^3', 'E(x)^4', 'V(x)', 'Skewness', 'Kurtosis'])
table_c.insert(0, 'c', np.round(cs, 3))

# === Display the tables ===
print("\nTable 1: Weighted Lognormal Moments, Variance , Skewness and Kurtosis\n")
print(table_sigma.round(6))

print("\nTable 2: Weighted Lognormal Moments, Variance , Skewness and Kurtosis\n")
print(table_mu.round(6))

print("\nTable 3: Weighted Lognormal Moments, Variance , Skewness and Kurtosis\n")
print(table_c.round(6))